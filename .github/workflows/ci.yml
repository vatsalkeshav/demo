name: k3s llama-api-server CI

on:
  push:
    branches: [main]
  # workflow_dispatch:
  # schedule:
  #   - cron: '0 0 * * *'

env:
  CARGO_TERM_COLOR: always

jobs:
  k3s-demo:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1
          cache: false

      - name: Install plugin
        run: |
          curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugins wasi_nn-ggml -v 0.14.1

      - name: Install runwasi-wasmedge-shim
        run: |
          cd
          git clone https://github.com/containerd/runwasi.git
          cd runwasi
          ./scripts/setup-linux.sh
          make build-wasmedge
          INSTALL="sudo install" LN="sudo ln -sf" make install-wasmedge
          source ~/.bashrc
          which containerd-shim-wasmedge-v1

      - name: Install k3s (Kubernetes lightweight)
        run: |
          cd
          curl -sfL https://get.k3s.io | sh -
          sudo chmod 777 /etc/rancher/k3s/k3s.yaml # hack

      - name: Build the llama-api-server image and import it to k3s containerd
        run: |
          cd
          git clone --recurse-submodules https://github.com/second-state/runwasi-wasmedge-demo.git
          cd runwasi-wasmedge-demo

          sed -i -e '/define CHECK_CONTAINERD_VERSION/,/^endef/{
          s/Containerd version must be/WARNING: Containerd version should be/
          /exit 1;/d
          }' Makefile

          git -C apps/llamaedge apply $PWD/disable_wasi_logging.patch
          OPT_PROFILE=release RUSTFLAGS="--cfg wasmedge --cfg tokio_unstable" make apps/llamaedge/llama-api-server
          cd apps/llamaedge/llama-api-server
          oci-tar-builder --name llama-api-server \
            --repo ghcr.io/second-state \
            --tag latest \
            --module target/wasm32-wasip1/release/llama-api-server.wasm \
            -o target/wasm32-wasip1/release/img-oci.tar
          sudo k3s ctr image import --all-platforms target/wasm32-wasip1/release/img-oci.tar
          sudo k3s ctr images ls

      - name: Download gguf model
        run: |
          sudo mkdir -p /home/runner/models
          sudo chmod 777 /home/runner/models
          cd /home/runner/models
          curl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf

      - name: Create and apply Kubernetes deployment
        run: |
          cd
          touch deployment.yaml
          sudo tee deployment.yaml > /dev/null <<'EOF'
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: llama-api-server
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: llama-api-server
            template:
              metadata:
                labels:
                  app: llama-api-server
              spec:
                runtimeClassName: wasmedge
                containers:
                  - name: llama-api-server
                    image: ghcr.io/second-state/llama-api-server:latest
                    imagePullPolicy: Never
                    command: ["llama-api-server.wasm"]
                    args:
                      - "--prompt-template"
                      - "llama-3-chat"
                      - "--ctx-size"
                      - "4096"
                      - "--model-name"
                      - "llama-3-1b"
                    env:
                      - name: WASMEDGE_PLUGIN_PATH
                        value: "/home/runner/.wasmedge/plugin"

                      - name: LD_LIBRARY_PATH
                        value: "/home/runner/.wasmedge/lib"

                      - name: WASMEDGE_WASINN_PRELOAD
                        value: "default:GGML:CPU:/home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf"
                    
                    volumeMounts:
                      - name: gguf-model-file
                        mountPath: /home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
                        readOnly: true

                      - name: wasi-nn-plugin-file
                        mountPath: /home/runner/.wasmedge/plugin/libwasmedgePluginWasiNN.so
                        readOnly: true

                      - name: wasi-nn-plugin-lib
                        mountPath: /home/runner/.wasmedge/lib
                        readOnly: true

                      - name: libm
                        mountPath: /lib/x86_64-linux-gnu/libm.so.6
                        readOnly: true
                      - name: libpthread
                        mountPath: /lib/x86_64-linux-gnu/libpthread.so.0
                        readOnly: true
                      - name: libc
                        mountPath: /lib/x86_64-linux-gnu/libc.so.6
                        readOnly: true
                      - name: ld-linux
                        mountPath: /lib64/ld-linux-x86-64.so.2
                        readOnly: true
                      - name: libdl
                        mountPath: /lib/x86_64-linux-gnu/libdl.so.2
                        readOnly: true
                      - name: libstdcxx
                        mountPath: /lib/x86_64-linux-gnu/libstdc++.so.6
                        readOnly: true
                      - name: libgcc-s
                        mountPath: /lib/x86_64-linux-gnu/libgcc_s.so.1
                        readOnly: true

                volumes:
                  - name: gguf-model-file
                    hostPath:
                      path: /home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
                      type: File
                  - name: wasi-nn-plugin-file
                    hostPath:
                      path: /home/runner/.wasmedge/plugin/libwasmedgePluginWasiNN.so
                      type: File
                  - name: wasi-nn-plugin-lib
                    hostPath:
                      path: /home/runner/.wasmedge/lib
                      type: Directory

                  - name: libm
                    hostPath:
                      path: /lib/x86_64-linux-gnu/libm.so.6
                      type: File
                  - name: libpthread
                    hostPath:
                      path: /lib/x86_64-linux-gnu/libpthread.so.0
                      type: File
                  - name: libc
                    hostPath:
                      path: /lib/x86_64-linux-gnu/libc.so.6
                      type: File
                  - name: ld-linux
                    hostPath:
                      path: /lib64/ld-linux-x86-64.so.2
                      type: File
                  - name: libdl
                    hostPath:
                      path: /lib/x86_64-linux-gnu/libdl.so.2
                      type: File
                  - name: libstdcxx
                    hostPath:
                      path: /lib/x86_64-linux-gnu/libstdc++.so.6
                      type: File
                  - name: libgcc-s
                    hostPath:
                      path: /lib/x86_64-linux-gnu/libgcc_s.so.1
                      type: File

          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: llama-api-server-service
          spec:
            selector:
              app: llama-api-server
            ports:
              - protocol: TCP
                port: 8080
                targetPort: 8080
            type: ClusterIP

          ---
          apiVersion: node.k8s.io/v1
          kind: RuntimeClass
          metadata:
            name: wasmedge
          handler: wasmedge
          EOF
          sudo k3s kubectl apply -f deployment.yaml
          sudo k3s kubectl get pods

      # - name: Start tmate session (SSH)
      #   uses: mxschmitt/action-tmate@v3

      - name: Port-forward in background
        run: |
          sudo k3s kubectl port-forward svc/llama-api-server-service 8080:8080 > /dev/null 2>&1 &
          echo "Waiting for port-forward to be ready..."
          sleep 5  # give it some time to establish the connection
          echo "PID of port-forward process: $!"
          echo "PORTER_PID=$!" >> $GITHUB_ENV

      
      - name: Test API endpoint
        run: |
          curl -X POST http://localhost:8080/v1/chat/completions \
          -H 'accept:application/json' \
          -H 'Content-Type: application/json' \
          -d '{
              "messages": [
                  {"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": "Who is Robert Oppenheimer?"}
              ],
              "model": "llama-3-1b"
          }'
      
      - name: Create Logs Directory
        run: mkdir -p logs
      
      - name: Get Pod Name
        id: get-pod
        run: |
          sleep 10
          sudo k3s kubectl get pods
          POD_NAME=$(sudo k3s kubectl get pods -l app=llama-api-server --no-headers -o custom-columns=":metadata.name" | head -n 1)
          echo "POD_NAME=$POD_NAME" >> $GITHUB_ENV

      - name: Display as well as save pod logs
        run: |
          sleep 30
          sudo k3s kubectl logs ${{ env.POD_NAME }}
          sudo k3s kubectl logs ${{ env.POD_NAME }} > "logs/pod_logs_$(date +'%Y-%m-%d').log"

      - name: Test API again and Save Output
        run: |
          curl -X POST http://localhost:8080/v1/chat/completions \
               -H 'accept:application/json' \
               -H 'Content-Type: application/json' \
               -d '{
                   "messages": [
                       {"role": "system", "content": "You are a helpful assistant."},
                       {"role": "user", "content": "Who is Robert Oppenheimer?"}
                   ],
                   "model": "llama-3-1b"
               }' > "logs/api_response_$(date +'%Y-%m-%d').json"

      - name: Upload Logs as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: daily-logs-${{ github.run_number }}
          path: logs/

      # # Optional: Test the API (port-forward and curl)
      # - name: Test llama-api-server endpoint
      #   run: |
      #     sleep 300
      #     sudo k3s kubectl port-forward svc/llama-api-server-service 8080:8080 &
      #     sleep 1
      #     curl -X POST http://localhost:8080/v1/chat/completions \
      #       -H 'accept:application/json' \
      #       -H 'Content-Type: application/json' \
      #       -d '{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Who is Robert Oppenheimer?"}], "model": "llama-3-1b"}'

      # - name: Cleanup
      #   if: always()
      #   run: |
      #     sudo k3s kubectl delete -f deployment.yaml || true
