name: k3s llama-api-server CI

on:
  push:
    branches: [main]
  # workflow_dispatch:
  # schedule:
  #   - cron: '0 0 * * *'

env:
  CARGO_TERM_COLOR: always

jobs:
  k3s-demo:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # - name: Install system dependencies
      #   run: |
      #     sudo apt-get update && sudo apt-get upgrade -y
      #     sudo apt-get install -y llvm-14-dev liblld-14-dev software-properties-common gcc g++ asciinema containerd cmake zlib1g-dev build-essential python3 python3-dev python3-pip git clang make pkgconf libtool libsystemd-dev libprotobuf-c-dev libcap-dev libseccomp-dev libyajl-dev go-md2man autoconf automake

      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1
          cache: false

      - name: Install plugin
        run: |
          curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugins wasi_nn-ggml -v 0.14.1
      #    echo "source $HOME/.bashrc" >> $GITHUB_ENV

      # - name: Install crun with WasmEdge support
      #   run: |
      #     git clone https://github.com/containers/crun
      #     cd crun
      #     ./autogen.sh
      #     ./configure --with-wasmedg
      #     make
      #     sudo make install

      - name: Install runwasi-wasmedge-shim
        run: |
          git clone https://github.com/containerd/runwasi.git
          cd runwasi
          ./scripts/setup-linux.sh
          make build-wasmedge
          INSTALL="sudo install" LN="sudo ln -sf" make install-wasmedge

      - name: Install k3s (Kubernetes lightweight)
        run: |
          curl -sfL https://get.k3s.io | sh -
          sudo chmod 777 /etc/rancher/k3s/k3s.yaml

      # - name: Configure containerd to use crun as default OCI runtime
      #   run: |
      #     sudo chmod 777 -R /var
      #     sudo tee /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl > /dev/null <<'EOF'
      #     version = 3
      #     root = "/var/lib/rancher/k3s/agent/containerd"
      #     state = "/run/k3s/containerd"
      #     [grpc]
      #       address = "/run/k3s/containerd/containerd.sock"
      #     [plugins.'io.containerd.internal.v1.opt']
      #       path = "/var/lib/rancher/k3s/agent/containerd"
      #     [plugins.'io.containerd.grpc.v1.cri']
      #       stream_server_address = "127.0.0.1"
      #       stream_server_port = "10010"
      #     [plugins.'io.containerd.cri.v1.runtime']
      #       enable_selinux = false
      #       enable_unprivileged_ports = true
      #       enable_unprivileged_icmp = true
      #       device_ownership_from_security_context = false
      #       default_runtime_name = "crun"
      #     [plugins.'io.containerd.cri.v1.images']
      #       snapshotter = "overlayfs"
      #       disable_snapshot_annotations = true
      #     [plugins.'io.containerd.cri.v1.images'.pinned_images]
      #       sandbox = "rancher/mirrored-pause:3.6"
      #     [plugins.'io.containerd.cri.v1.runtime'.cni]
      #       bin_dir = "/var/lib/rancher/k3s/data/cni"
      #       conf_dir = "/var/lib/rancher/k3s/agent/etc/cni/net.d"
      #     [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc]
      #       runtime_type = "io.containerd.runc.v2"
      #     [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc.options]
      #       SystemdCgroup = true
      #     [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runhcs-wcow-process]
      #       runtime_type = "io.containerd.runhcs.v1"
      #     [plugins.'io.containerd.cri.v1.runtime'.containerd]
      #       default_runtime_name = "crun"
      #       [plugins."io.containerd.cri.v1.runtime".containerd.runtimes]
      #         [plugins."io.containerd.cri.v1.runtime".containerd.runtimes.crun]
      #           runtime_type = "io.containerd.runc.v2"
      #           [plugins."io.containerd.cri.v1.runtime".containerd.runtimes.crun.options]
      #             BinaryName = "/usr/local/bin/crun"
      #             SystemdCgroup = true
      #     [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.wasmedge]
      #       runtime_type = "io.containerd.wasmedge.v1"
      #       privileged_without_host_devices = true
      #       [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.wasmedge.options]
      #         BinaryName = "/usr/local/bin/containerd-shim-wasmedge-v1"
      #         SystemdCgroup = true
      #     [plugins.'io.containerd.cri.v1.images'.registry]
      #       config_path = "/var/lib/rancher/k3s/agent/etc/containerd/certs.d"
      #     EOF
      #     sudo systemctl restart k3s

      - name: Build llama-api-server image and import to k3s containerd
        run: |
          git clone --recurse-submodules https://github.com/second-state/runwasi-wasmedge-demo.git
          cd runwasi-wasmedge-demo

          sed -i -e '/define CHECK_CONTAINERD_VERSION/,/^endef/{
          s/Containerd version must be/WARNING: Containerd version should be/
          /exit 1;/d
          }' Makefile

          git -C apps/llamaedge apply $PWD/disable_wasi_logging.patch
          OPT_PROFILE=release RUSTFLAGS="--cfg wasmedge --cfg tokio_unstable" make apps/llamaedge/llama-api-server
          cd apps/llamaedge/llama-api-server
          oci-tar-builder --name llama-api-server \
            --repo ghcr.io/second-state \
            --tag latest \
            --module target/wasm32-wasip1/release/llama-api-server.wasm \
            -o target/wasm32-wasip1/release/img-oci.tar
          sudo k3s ctr image import --all-platforms target/wasm32-wasip1/release/img-oci.tar
          sudo k3s ctr images ls

      - name: Download gguf model
        run: |
          sudo mkdir -p /home/runner/models
          sudo chmod 777 /home/runner/models
          cd /home/runner/models
          curl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf

      - name: Create and apply Kubernetes deployment
        run: |
          cat <<'EOF' > deployment.yaml
          # (deployment YAML from README goes here, update paths for /home/runner)
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: llama-api-server
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: llama-api-server
            template:
              metadata:
                labels:
                  app: llama-api-server
              spec:
                runtimeClassName: wasmedge
                containers:
                  - name: llama-api-server
                    image: ghcr.io/second-state/llama-api-server:latest
                    imagePullPolicy: Never
                    command: ["llama-api-server.wasm"]
                    args:
                      - "--prompt-template"
                      - "llama-3-chat"
                      - "--ctx-size"
                      - "4096"
                      - "--model-name"
                      - "llama-3-1b"
                    env:
                      - name: WASMEDGE_PLUGIN_PATH
                        value: "/home/runner/.wasmedge/plugin"
                      - name: LD_LIBRARY_PATH
                        value: "/home/runner/.wasmedge/lib"
                      - name: WASMEDGE_WASINN_PRELOAD
                        value: "default:GGML:CPU:/home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf"
                    volumeMounts:
                      - name: gguf-model-file
                        mountPath: /home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
                        readOnly: true
                      - name: wasi-nn-plugin-file
                        mountPath: /home/runner/.wasmedge/plugin/libwasmedgePluginWasiNN.so
                        readOnly: true
                      - name: wasi-nn-plugin-lib
                        mountPath: /home/runner/.wasmedge/lib
                        readOnly: true
                volumes:
                  - name: gguf-model-file
                    hostPath:
                      path: /home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
                      type: File
                  - name: wasi-nn-plugin-file
                    hostPath:
                      path: /home/runner/.wasmedge/plugin/libwasmedgePluginWasiNN.so
                      type: File
                  - name: wasi-nn-plugin-lib
                    hostPath:
                      path: /home/runner/.wasmedge/lib
                      type: Directory
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: llama-api-server-service
          spec:
            selector:
              app: llama-api-server
            ports:
              - protocol: TCP
                port: 8080
                targetPort: 8080
            type: ClusterIP
          ---
          apiVersion: node.k8s.io/v1
          kind: RuntimeClass
          metadata:
            name: wasmedge
          handler: wasmedge
          EOF
          sudo k3s kubectl apply -f deployment.yaml
          sudo k3s kubectl get pods

      - name: Wait for pod to be ready
        run: |
          for i in {1..1000}; do
            STATUS=$(sudo k3s kubectl get pods -l app=llama-api-server -o jsonpath='{.items[0].status.phase}')
            if [ "$STATUS" = "Running" ]; then
              echo "Pod is running!"
              break
            fi
            echo "Waiting for pod... ($i)"
            sleep 10
          done

      # Optional: Test the API (port-forward and curl)
      - name: Test llama-api-server endpoint
        run: |
          sleep 300
          sudo k3s kubectl port-forward svc/llama-api-server-service 8080:8080 &
          sleep 1
          curl -X POST http://localhost:8080/v1/chat/completions \
            -H 'accept:application/json' \
            -H 'Content-Type: application/json' \
            -d '{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Who is Robert Oppenheimer?"}], "model": "llama-3-1b"}'

      # - name: Cleanup
      #   if: always()
      #   run: |
      #     sudo k3s kubectl delete -f deployment.yaml || true


